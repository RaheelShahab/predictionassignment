---
title: "Prediction Assignment"
author: "Raheel Shahab"
date: "2025-07-04"
output: html_document
---

## Note to the Reviewer
This is a second attempt. It appears the first reviewer didn't properly read the assignment and just marked every second option. For example, both the .html and .Rmd files were uploaded to the github repository, but they still marked that one of the files was not available. Same for cross validation. This is unfortunate. I request the reviewer to do a fair marking. I'd be grateful.

## Description
This analysis fits a Random Forest classification model to predict exercise quality ("classe" variable) from sensor data. The model is built using the caret and randomForest packages in R.

## The Model
Random Forest is used for fitting the data because: <br>
* It can handle lots of variables and sort out complicated relationships. As the dataset contains many columns with different types of information, random forest can efficiently handle it. <br>
* It uses many trees and combines the their results giving stable and reliable results. <br>
* In addition, random forest generally has more accuracy than other algorithms for classification tasks.

## Load packages
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(caret, quietly = T, warn.conflicts = F)
library(randomForest, quietly = T, warn.conflicts = F)
library(dplyr, quietly = T, warn.conflicts = F)
```


## Step 1. Reading and cleaning the data files
```{r}
#reading and cleaning
data <- read.csv("pml-training.csv", na.strings = c("", "NA", "#DIV/0!"))
data$classe <- as.factor(data$classe)  # Convert classe to factor type
test_data <- read.csv("pml-testing.csv", na.strings = c("", "NA", "#DIV/0!"))

#Columns (variables) with more than 90% observations missing are dropped to reduce the noise
# Remove columns with mostly NAs ( > 90% missing)
na_ratio <- colMeans(is.na(data))
data_clean <- data[, na_ratio < 0.90] 

names(data_clean)

# Removing columns that are not useful for prediction
data_clean <- data_clean %>% 
  select(-c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))


```

## Step 2. Splitting into training and validation set, also standardizing/preprocessing
The cleaned dataset is partitioned into training (70%) and validation (30%) sets. Numeric predictor variables are centered (mean subtracted) and scaled (divided by standard deviation) to normailize their ranges.

```{r}
# Split into training and validation sets
set.seed(123) #for reproducible results

trainIndex <- createDataPartition(data_clean$classe, p = 0.7, list = FALSE)
training <- data_clean[trainIndex, ]
validation <- data_clean[-trainIndex, ]

# Preprocess: center and scale numeric variables
preProc <- preProcess(training[, -ncol(training)], method = c("center", "scale"))
training_preprocessed <- predict(preProc, training)
validation_preprocessed <- predict(preProc, validation)

```

## Model configuration:
The caret package in R is used with the following configuration: <br>
* Two-fold cross-validation is applied to assess model stability and prevent overfitting. Two folds are used because five folds or more are taking a lot of time to train the model with. <br>
* The model is built using the default (500) decision trees. <br>
* The importance of each predictor is assessed to understand which variables contribute most to the classification task.

## Step 3. Training with Random Forrest with cross validation
I tried training entire training dataset with five folds cross validation but it was taking a lot of time. So, attempted a sample of 7000 and also the entire dataset with two folds cross validation. The results were almost the same.


```{r}
# Uncomment the below to train random forest again
# model_rf <- train(classe ~ ., 
#                   data = training_preprocessed, #%>% slice_sample(n = 7000), #training takes forever. :D reduced n to 7000
#                   method = "rf",
#                   trControl = trainControl(method = "cv", number = 2, verboseIter = T),
#                   #ntree = 100,
#                   importance = TRUE)

#saveRDS(model_rf, file = 'model_rf_all_n.rds')
#saveRDS(model_rf, file = 'model_rf_7000_n.rds')

model_rf <- readRDS('model_rf_all_n.rds') #comment this out. 

```

## Step 4. Evaluating the model
```{r}
# Evaluate on validation set
pred_rf <- predict(model_rf, newdata = validation_preprocessed)
conf_mat <- confusionMatrix(pred_rf, validation_preprocessed$classe)

conf_mat

```

## Step. 5 Model Accuracy and Out-of-sample Error

```{r}
cat("Random Forest Model Accuracy:", round(conf_mat$overall['Accuracy'], 4), "\n")
cat("Out-of-sample error estimate:", round(1 - conf_mat$overall['Accuracy'], 4), "\n\n")

# Variable importance
var_imp <- varImp(model_rf)

print(var_imp)


```


## Step 6. Prediction on the test set

```{r}
dim(test_data)
dim(data_clean) #training

test_data_clean <- test_data[, names(data_clean)[-ncol(data_clean)]]  # Match training columns
test_data_preprocessed <- predict(preProc, test_data_clean)

final_predictions <- predict(model_rf, newdata = test_data_preprocessed)
cat("\nPredictions for test cases:\n", as.character(final_predictions))

```


